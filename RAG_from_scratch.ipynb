{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPk4U9LF0IMhfgA6yPmCWE+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keyaaness/RAG-from-Scratch/blob/main/RAG_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WWpnzyLX1QCI"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "import os\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import uuid\n",
        "import torch\n",
        "import numpy as np\n",
        "import json\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luDX5YQU_Qc2",
        "outputId": "2cec2a2c-3e0a-4b3c-c883-8da05500fdd2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1 : chunking\n",
        "def chunking(directory_path, tokenizer, chunk_size, para_seperator=\" /n /n\", separator=\" \"):\n",
        "    documents = {}\n",
        "    all_chunks = {}\n",
        "    for filename in os.listdir(directory_path):\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "        print(filename)\n",
        "        base = os.path.basename(file_path)\n",
        "        sku = os.path.splitext(base)[0]\n",
        "        if os.path.isfile(file_path):\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                text = file.read()\n",
        "\n",
        "            doc_id = str(uuid.uuid4())\n",
        "\n",
        "            paragraphs = re.split(para_seperator, text)\n",
        "\n",
        "            for paragraph in paragraphs:\n",
        "                words = paragraph.split(separator)\n",
        "                current_chunk_str = \"\"\n",
        "                chunk = []\n",
        "                for word in words:\n",
        "                    if current_chunk_str:\n",
        "                        new_chunk = current_chunk_str + separator + word\n",
        "                    else:\n",
        "                        new_chunk = current_chunk_str + word\n",
        "                    if len(tokenizer.tokenize(new_chunk)) <= chunk_size:\n",
        "                        current_chunk_str = new_chunk\n",
        "                    else:\n",
        "                        if current_chunk_str:\n",
        "                            chunk.append(current_chunk_str)\n",
        "                        current_chunk_str = word\n",
        "\n",
        "                if current_chunk_str:\n",
        "                    chunk.append(current_chunk_str)\n",
        "\n",
        "                for chunk in chunk:\n",
        "                    chunk_id = str(uuid.uuid4())\n",
        "                    all_chunks[chunk_id] = {\"text\": chunk, \"metadata\": {\"file_name\": sku}}\n",
        "        documents[doc_id] = all_chunks\n",
        "    return documents"
      ],
      "metadata": {
        "id": "L7gqKVno2FLx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 2 : generating embeddings\n",
        "def map_document_embeddings(documents, tokenizer, model):\n",
        "    mapped_document_db = {}\n",
        "    for id, dict_content in documents.items():\n",
        "        mapped_embeddings = {}\n",
        "        for content_id, text_content in dict_content.items():\n",
        "            text = text_content.get(\"text\")\n",
        "            inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "            with torch.no_grad():\n",
        "                embeddings = model(**inputs).last_hidden_state.mean(dim=1).squeeze().tolist()\n",
        "            mapped_embeddings[content_id] = embeddings\n",
        "        mapped_document_db[id] = mapped_embeddings\n",
        "    return mapped_document_db"
      ],
      "metadata": {
        "id": "imwod5dO-385"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_json(path):\n",
        "    with open(path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return data"
      ],
      "metadata": {
        "id": "_2lB7qPc_7k-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_json(path, data):\n",
        "    with open(path, 'w') as f:\n",
        "        json.dump(data, f, indent=4)"
      ],
      "metadata": {
        "id": "g8OGpqyV6mrF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    drive_base_path = \"/content/drive/My Drive\"\n",
        "    documents_folder = os.path.join(drive_base_path, \"documents\")\n",
        "    database_folder = os.path.join(drive_base_path, \"database\")\n",
        "    os.makedirs(database_folder, exist_ok=True)\n",
        "\n",
        "    model_name = \"BAAI/bge-small-en-v1.5\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "    chunk_size = 200\n",
        "    para_seperator = \" /n /n\"\n",
        "    separator = \" \"\n",
        "    top_k = 2\n",
        "    openai_model = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key='your_api_key')  # Add your API key\n",
        "\n",
        "    # create document store with chunk id, doc_id, text\n",
        "    documents = chunking(documents_folder, tokenizer, chunk_size, para_seperator, separator)\n",
        "\n",
        "    # now embed and map to database\n",
        "    mapped_document_db = map_document_embeddings(documents, tokenizer, model)\n",
        "\n",
        "    # save JSON to google drive\n",
        "    save_json(os.path.join(database_folder, \"doc_store_2.json\"), documents)\n",
        "    save_json(os.path.join(database_folder, \"vector_store_2.json\"), mapped_document_db)\n",
        "\n",
        "    # retrieving most relevant data chunks\n",
        "    query = \"why toddlers throw tantrums?\"\n",
        "    query_embeddings = compute_embeddings(query, tokenizer, model)\n",
        "    sorted_scores = retrieve_top_k_scores(query_embeddings, mapped_document_db, top_k)\n",
        "    top_results = retrieve_top_results(sorted_scores)\n",
        "\n",
        "    # reading JSON\n",
        "    document_data = read_json(os.path.join(database_folder, \"doc_store_2.json\"))  # read document store\n",
        "\n",
        "    # retrieve text of relevant chunk embeddings\n",
        "    relevant_text = retrieve_text(top_results, document_data)\n",
        "\n",
        "    print(relevant_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4SomcS7_qCP",
        "outputId": "4143e39d-3d82-4e08-8df9-94d723fffee6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "behaviuor1.txt\n",
            "behaviuor2.txt\n",
            "behaviuor3.txt\n",
            "{'text': \"\\n\\nChildren throw tantrums when they are overwhelmed by strong emotions, overstimulated, or anxious. Young children may have tantrums because of strong emotions, while older children may have tantrums because they don't know how to express or manage their feelings. Overstimulation can also cause tantrums, as children don't always recognize when bright lights or loud noises are bothering them. Anxiety can also cause tantrums, such as when a child feels anxious due to an unexpected event, unrealistic demand, or lack of routine.\\n \\n\\nTantrums may happen when kids are tired, hungry, or uncomfortable. They can have a meltdown because they can't have something they want (like a toy or candy) or canâ€™t get someone to do what they want (like getting a parent to pay attention to them immediately or getting a sibling to give up the tablet). Learning to deal with frustration is a skill that children gain over\", 'metadata': {'file_name': 'behaviuor1'}}\n"
          ]
        }
      ]
    }
  ]
}